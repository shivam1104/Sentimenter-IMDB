{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the required packages. It is necessary to first install the following packages:  \n",
    "  \n",
    "`pip install pandas`  \n",
    "`pip install numpy`  \n",
    "`pip install nltk`  \n",
    "  \n",
    "To install TensorFlow on CPU:  \n",
    "`pip install tensorflow`  \n",
    "To install TensorFlow on GPU:  \n",
    "`pip install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_y = np.eye(2)[train.labels[:20000]] # One-hot encode the labels\n",
    "val_y = np.eye(2)[train.labels[20000:]] # One-hot encode the labels\n",
    "trn_txt = train.text[:20000]\n",
    "val_txt = train.text[20000:]\n",
    "tst_txt = test.text\n",
    "texts = np.hstack([trn_txt, val_txt, tst_txt]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cleaning text and performing stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    re1 = re.compile(r'  +')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    x = ' '.join([stemmer.stem(word) for word in str(x).split(' ')])\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it\\'s also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [stem(txt) for txt in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a extrem well-mad film. the acting, script and camera-work are all first-rate. the music is good, too, though it is most earli in the film, when thing are still relat cheery. there are no realli superstar in the cast, though sever face will be familiar. the entir cast doe an excel job with the script.\\n\\nbut it is hard to watch, becaus there is no good end to a situat like the one presented. it is now fashion to blame the british for set hindus and muslim against each other, and then cruelli separ them into two countries. there is some merit in this view, but it also true that no one forc hindus and muslim in the region to mistreat each other as they did around the time of partition. it seem more like that the british simpli saw the tension between the religion and were clever enough to exploit them to their own ends.\\n\\nthe result is that there is much cruelti and inhuman in the situat and this is veri unpleas to rememb and to see on the screen. but it is never paint as a black-and-whit case. there is base and nobil on both sides, and also the hope for chang in the younger generation.\\n\\nthere is redempt of a sort, in the end, when puro has to make a hard choic between a man who has ruin her life, but also truli love her, and her famili which has disown her, then later come look for her. but by that point, she has no option that is without great pain for her.\\n\\nthis film carri the messag that both muslim and hindus have their grave faults, and also that both can be dignifi and care people. the realiti of partit make that realis all the more wrenching, sinc there can never be real reconcili across the india/pakistan border. in that sense, it is similar to \"mr & mrs iyer\".\\n\\nin the end, we were glad to have seen the film, even though the resolut was heartbreaking. if the uk and us could deal with their own histori of racism with this kind of frankness, they would certain be better off.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an integer token for each word and apply the tokenizer to the datasets. For more information on Tensorflow/Keras for text processing see:  \n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 5000\n",
    "t = Tokenizer(n_words)\n",
    "t.fit_on_texts(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_seq = t.texts_to_sequences([stem(txt) for txt in trn_txt])\n",
    "val_seq = t.texts_to_sequences([stem(txt) for txt in val_txt])\n",
    "tst_seq = t.texts_to_sequences([stem(txt) for txt in tst_txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep up to 300 words of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 300\n",
    "trn_seq = np.array(pad_sequences(trn_seq, max_words))\n",
    "val_seq = np.array(pad_sequences(val_seq, max_words))\n",
    "tst_seq = np.array(pad_sequences(tst_seq, max_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first sentence (converted to an array of integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1034,    1,  360,  165,  138,   32,  404,  298,   16,    1,  218,\n",
       "         17,    7,    6,  215,    5,   56,   94,   37,    6,   58,   47,\n",
       "         96,    5,    3,  786,   30,    1,   27,    7,    6,  156, 1133,\n",
       "          5, 1363,    1,  714,   15,  191,    2, 4049,  486,  276,   74,\n",
       "          2,  101, 1827,  100,   89,  114,   37,    6,   46, 2596,    8,\n",
       "         10,  366,   17,    7,   87,  303,   11,   58,   27,  510,    2,\n",
       "       4049,    8,    1, 3781,    5,  276,   74,   13,   34,  124,  192,\n",
       "          1,   49,    4,    7,  110,   51,   30,   11,    1,  714,  374,\n",
       "        217,    1, 1055,  209,    1, 2026,    2,   72,  915,  202,    5,\n",
       "       1402,  100,    5,   64,  201, 2696,    1,  659,    6,   11,   37,\n",
       "          6,   78,    2,    8,    1,  786,    2,   10,    6,   54, 3772,\n",
       "          5,  385,    2,    5,   53,   19,    1,  254,   17,    7,    6,\n",
       "        118, 1259,   13,    3,  316,    2, 4320,  431,   37,    6,  455,\n",
       "          2,   19,  204,    2,   87,    1,  290,   15,  407,    8,    1,\n",
       "       1130, 3987,   37,    6, 4824,    4,    3,  426,    8,    1,   96,\n",
       "         50,   44,    5,   48,    3,  215, 1135,  209,    3,  131,   33,\n",
       "         44, 1147,   40,  126,   17,   87,  412,   90,   40,    2,   40,\n",
       "        293,   63,   44,   40,  101,  323,  121,   76,   15,   40,   17,\n",
       "         31,   11,  178,   57,   44,   58, 4000,   11,    6,  214,   85,\n",
       "        647,   15,   40,   10,   14,  708,    1,  888,   11,  204, 4049,\n",
       "          2,   23,   64, 2071,    2,   87,   11,  204,   67,   20,    2,\n",
       "        359,  509,    1,  865,    4,   48,   11, 1951,   29,    1,   51,\n",
       "        257,   37,   67,  118,   20,  152,  635,    1, 2419, 2614,    8,\n",
       "         11,  969,    7,    6,  629,    5,  473, 1812,    8,    1,   96,\n",
       "         80,   72, 1173,    5,   23,  115,    1,   14,   59,  161,    1,\n",
       "       4001,   12,   43,    1, 2159,    2,  183,  106,  516,   16,   64,\n",
       "        201,  767,    4, 2597,   16,   10,  232,    4,   34,   61,  304,\n",
       "         20,  134,  127])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_seq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network with Keras to predict sentiment from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each word as 64 numbers, put the sequence through an LSTM Neural Network. For more information see: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(n_words, 64, input_length = max_words, input_shape=(max_words,)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr=.01), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 64)           320000    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 353,666\n",
      "Trainable params: 353,410\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andchristiansen\\AppData\\Local\\Continuum\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 0.5358 - acc: 0.7210 - val_loss: 0.4191 - val_acc: 0.8146\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 210s 11ms/step - loss: 0.3766 - acc: 0.8331 - val_loss: 0.4082 - val_acc: 0.8232\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 0.3304 - acc: 0.8593 - val_loss: 0.3614 - val_acc: 0.8474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25af221d0b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_seq,\n",
    "          trn_y,\n",
    "          validation_data = [val_seq, val_y],\n",
    "          epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentiment for each review in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tst_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most likely to be negative sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This movie is just plain dumb. Don't bother watching it; believe me, you're better off.<br /><br />Long and short of the plot: a defense attorney represents a man who murdered his son and other children. In defending him, she comes across a wooden doll of Pinnochio. She takes the doll home. Pinnochio is possessed and begins to start killing people.<br /><br />This movie moves very slowly only to have such a weak ending. The plot is very bad and the Dennis Michael Tenney's musical score is pitiful. The story, written by Kevin S. Tenney, is just pointless and evokes NO horror or fear. This is a far cry from his work on Night of the Demons and Witchboard, which are decent outings but nothing to write home about. His directing is OK, but with such a bad story no one could have made this movie any good.<br /><br />In conclusion: 2 out of 10, perhaps the blandest, most boring movie I've seen all year.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text.iloc[np.argmax(preds[:,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most likely to be positive sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Tourist Trap\" is a bizarre, great horror film from the \\'70s. The film is about a group of young adults, Becky, Jerry, and Molly, who are traveling in a jeep through a desert area. Their two other friends, Eileen and her boyfriend Woody, are in a separate car. When a wheel goes flat, Woody takes it to a nearby gas station - and meets a grisly fate to some bizarre telekinetic mayhem and some creepy mannequins. The friends get tired of waiting for Woody and go to a local \"tourist trap\" mannequin/wax museum. In front of the entrance, the car randomly breaks down, and the girls find an oasis area to go swimming in, where they are approached by Mr. Slausen, who runs the roadside attraction that is now closed down. He takes them up to the old western wax museum, and the girls stay behind while he and Jerry go to fix their car. Eileen, the curious of the two, wanders to an old house nearby, where she also falls to the hands of a mysterious masked killer and a bunch of life like mannequins. After awaiting for Eileen, Becky and Molly go to look for her. That\\'s when the real horror begins, and the telekinetic (can move objects with his mind) masked brother of Mr. Slausen begins to kill off the teens one by one, while controlling his large amount of human-turned mannequins.<br /><br />Sound similar to the 2005 \"House of Wax\" remake? Well, it is. I\\'d heard of this movie but never seen it when I saw \"House of Wax\", but now I can see the striking similarities the two movie share - \"Tourist Trap\" was obviously a big contributor to the \"House of Wax\" remake. The mannequins in this movie are scary to begin with, some with moving eyeballs, some with no eyes at all, and some with dropping mouths that sing too. The singing was extremely creepy if you ask me, and the mannequins were eerily designed. Mannequins are creepy to begin with, they\\'re so lifelike yet they really aren\\'t. The movie tightly blends elements from \"Texas Chainsaw Massacre\" (the masks the killer uses are similar to those of Leatherface), with a little bit of the original \"House of Wax\", and the telekinetic powers that are displayed in \"Carrie\". The result is quite satisfying. The telekinesis was a nice touch to the movie, it made the killer all the more menacing and inescapable. The masks were terrifying, and the plastering scene was really disturbing.<br /><br />The score for the film was really well done, if not a little overused during some scenes. The acting may not be particularly on key, but it really wasn\\'t that bad. Chuck Connors was really good as the shadowy Mr. Slauston, giving the character a shady but friendly feel. A young Tanya Roberts is also in the film, she\\'s most known for her role on \"Charlie\\'s Angels\" and more recently the sitcom \"That \\'70s Show\", playing Donna\\'s mother. Robert A. Burns serves as the art director, he did a phenomenal job on the original \"Texas Chainsaw\" and does a good job here as well, creating a cluttered, musty atmosphere to both the rundown museum and the old house filled with mannequins. I found it a little odd that the original rating for the film was PG, it seems a little too scary to have such a tame rating, but the film really isn\\'t too violent.<br /><br />Overall, \"Tourist Trap\" is an eerily unique, fast paced, extremely under-appreciated horror classic. Full Moon gave it a decent 20th anniversary DVD release, the commentary was interesting and the picture was clear and crisp for the most part, better than the video versions. If you enjoy older \\'70s slasher-horror films, \"Tourist Trap\" is an underrated retro gem. 8/10.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text.iloc[np.argmax(preds[:,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out some of our own reviews for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(strings):\n",
    "    if type(strings) is str:\n",
    "        strings = [strings]\n",
    "    seq = np.array(pad_sequences(t.texts_to_sequences([stem(string) for string in strings]),max_words))\n",
    "    pred = model.predict(seq)\n",
    "    for i in range(len(strings)):\n",
    "        print(\"%s  |  Positive Sentiment: %2.f%%\" % (strings[i], pred[i][1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |  Positive Sentiment: 53%\n"
     ]
    }
   ],
   "source": [
    "predict_words('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this movie! Great film  |  Positive Sentiment: 98%\n",
      "This movie is boring and terrible...  |  Positive Sentiment:  3%\n"
     ]
    }
   ],
   "source": [
    "predict_words(['I love this movie! Great film','This movie is boring and terrible...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly recommended  |  Positive Sentiment: 88%\n",
      "recommended  |  Positive Sentiment: 68%\n",
      "not recommended  |  Positive Sentiment: 59%\n"
     ]
    }
   ],
   "source": [
    "predict_words(['highly recommended','recommended','not recommended'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good  |  Positive Sentiment: 61%\n",
      "not good  |  Positive Sentiment: 32%\n",
      "bad  |  Positive Sentiment:  9%\n"
     ]
    }
   ],
   "source": [
    "predict_words(['good','not good','bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast pace  |  Positive Sentiment: 62%\n",
      "slow pace  |  Positive Sentiment: 17%\n",
      "very slow pace  |  Positive Sentiment: 19%\n"
     ]
    }
   ],
   "source": [
    "predict_words(['fast pace','slow pace','very slow pace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['labels'] = preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','labels']].to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
